# ğŸŒ¿ neurotwig

A minimal neural network engine built from scratch using pure Python â€” with automatic differentiation, graph visualization, and a micro MLP.

Inspired by Andrej Karpathy's [micrograd](https://github.com/karpathy/micrograd), `neurotwig` is educational, lightweight, and completely hackable.

---

## ğŸ§  Features

- `Value` class with backward propagation (autograd)
- Support for basic operations: `+`, `-`, `*`, `/`, `**`, `tanh`, `exp`
- Visualize computation graph using [Graphviz](https://graphviz.org/)
- Manual single neuron and multi-layer perceptron (MLP)
- Training loop with gradient descent
- No external ML libraries

---

## ğŸ“¦ Installation

Clone the repo:

```bash
git clone https://github.com/your-username/neurotwig.git
cd neurotwig
pip install -e .
```

> This will install `neurotwig` as an editable package for local development.

---

## ğŸ—‚ Project Structure

```
neurotwig/
â”œâ”€â”€ neurotwig/              # Core library code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ core.py             # Value class + autograd
â”‚   â”œâ”€â”€ net_struc.py        # Neuron, Layer, MLP
â”‚   â””â”€â”€ visualize.py        # Computation graph rendering
â”‚
â”œâ”€â”€ examples/               # Example scripts
â”‚   â”œâ”€â”€ basic_ops.py        # Basic math and graph visualization
â”‚   â”œâ”€â”€ manual_neuron.py    # Manual neuron with tanh
â”‚   â””â”€â”€ manual_back_prop.py # Manual back propogation
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ README.md
```

---

## ğŸš€ Examples

### Visualizing Basic Computation Graph

```python
from neurotwig.core import Value
from neurotwig.visualize import draw_dot

a = Value(2.0, label='a')
b = Value(-3.0, label='b')
c = a * b
c.label = 'c'

draw_dot(c).render('graph', format='png', cleanup=False)
```

### Training a Tiny Neural Network

```python
from neurotwig.model import MLP

mlp = MLP(3, [4, 4, 1])
x = [2.0, 3.0, -1.0]
out = mlp(x)
print(out)
```

---

## ğŸ“š Learn More

This project is perfect for:

- Understanding how backpropagation works
- Building your own neural networks
- Learning Python classes and autograd logic
- Visualizing how gradients flow

---

## ğŸ”– License

MIT License. See [LICENSE](LICENSE) for details.

---

## ğŸ™ Credits

Inspired by [micrograd](https://github.com/karpathy/micrograd) by [Andrej Karpathy](https://github.com/karpathy).
